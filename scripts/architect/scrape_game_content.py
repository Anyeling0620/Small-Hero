"""
æ¶æ„å¸ˆ - æ¸¸æˆå†…å®¹çˆ¬å–è„šæœ¬
æ¯æ—¥è‡ªåŠ¨çˆ¬å–å°å°å‹‡è€…ç›¸å…³çš„æ¸¸æˆèµ„è®¯ã€æ›´æ–°æ—¥å¿—ã€ç©å®¶åé¦ˆç­‰
"""
import os
import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import google.generativeai as genai

# é…ç½® Gemini API
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
genai.configure(api_key=GEMINI_API_KEY)

def scrape_taptap():
    """çˆ¬å– TapTap å°å°å‹‡è€…é¡µé¢"""
    print("ğŸ” æ­£åœ¨çˆ¬å– TapTap æ¸¸æˆä¿¡æ¯...")
    
    url = "https://www.taptap.cn/app/233851"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ¸¸æˆä¿¡æ¯
        game_info = {
            'source': 'TapTap',
            'url': url,
            'scraped_at': datetime.now().isoformat(),
            'description': soup.find('meta', {'name': 'description'})['content'] if soup.find('meta', {'name': 'description'}) else '',
            'raw_html': str(soup)[:5000]  # ä¿å­˜éƒ¨åˆ† HTML ä¾› AI åˆ†æ
        }
        
        return game_info
    except Exception as e:
        print(f"âŒ TapTap çˆ¬å–å¤±è´¥: {e}")
        return None

def scrape_reddit():
    """çˆ¬å– Reddit ç›¸å…³è®¨è®º"""
    print("ğŸ” æ­£åœ¨çˆ¬å– Reddit ç¤¾åŒºè®¨è®º...")
    
    url = "https://www.reddit.com/r/TinyRogues/top.json?limit=10"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        data = response.json()
        
        posts = []
        for post in data['data']['children']:
            post_data = post['data']
            posts.append({
                'title': post_data.get('title', ''),
                'content': post_data.get('selftext', ''),
                'score': post_data.get('score', 0),
                'url': f"https://www.reddit.com{post_data.get('permalink', '')}"
            })
        
        return {
            'source': 'Reddit',
            'scraped_at': datetime.now().isoformat(),
            'posts': posts
        }
    except Exception as e:
        print(f"âŒ Reddit çˆ¬å–å¤±è´¥: {e}")
        return None

def analyze_with_gemini(scraped_data):
    """ä½¿ç”¨ Gemini AI åˆ†æçˆ¬å–çš„å†…å®¹"""
    print("ğŸ¤– ä½¿ç”¨ Gemini AI åˆ†ææ¸¸æˆå†…å®¹...")
    
    model = genai.GenerativeModel('gemini-2.0-flash-exp')
    
    prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±æ¸¸æˆæ¶æ„å¸ˆï¼Œä¸“é—¨è´Ÿè´£åˆ†æå°å°å‹‡è€…ï¼ˆTiny Heroï¼‰æ¸¸æˆçš„æ ¸å¿ƒæœºåˆ¶ã€‚

è¯·åˆ†æä»¥ä¸‹çˆ¬å–çš„æ¸¸æˆä¿¡æ¯ï¼š

{json.dumps(scraped_data, ensure_ascii=False, indent=2)}

è¯·æå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
1. **æ ¸å¿ƒç©æ³•æœºåˆ¶**ï¼šæˆ˜æ–—ç³»ç»Ÿã€å‡çº§ç³»ç»Ÿã€è£…å¤‡ç³»ç»Ÿç­‰
2. **æ•°å€¼ç³»ç»Ÿ**ï¼šå±æ€§è®¡ç®—å…¬å¼ã€æˆé•¿æ›²çº¿
3. **UI/UX ç‰¹ç‚¹**ï¼šç•Œé¢é£æ ¼ã€äº¤äº’è®¾è®¡
4. **æœ€æ–°æ›´æ–°å†…å®¹**ï¼šæ–°å¢åŠŸèƒ½ã€æ”¹åŠ¨ç‚¹
5. **ç©å®¶åé¦ˆé‡ç‚¹**ï¼šç©å®¶æœ€å…³æ³¨çš„åŠŸèƒ½å’Œé—®é¢˜

ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸Š 5 ä¸ªå…³é”®ç‚¹ã€‚
"""
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"âŒ Gemini åˆ†æå¤±è´¥: {e}")
        return None

def save_report(data):
    """ä¿å­˜æ¯æ—¥ç ”ç©¶æŠ¥å‘Š"""
    today = datetime.now().strftime('%Y-%m-%d')
    report_dir = 'docs/game-research/daily-reports'
    os.makedirs(report_dir, exist_ok=True)
    
    report_path = f"{report_dir}/{today}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

def main():
    print("=" * 60)
    print("ğŸ—ï¸  æ¶æ„å¸ˆ - æ¯æ—¥æ¸¸æˆå†…å®¹çˆ¬å–")
    print("=" * 60)
    
    # çˆ¬å–å„ä¸ªæ¥æº
    scraped_data = {
        'date': datetime.now().strftime('%Y-%m-%d'),
        'sources': []
    }
    
    # TapTap
    taptap_data = scrape_taptap()
    if taptap_data:
        scraped_data['sources'].append(taptap_data)
    
    # Reddit
    reddit_data = scrape_reddit()
    if reddit_data:
        scraped_data['sources'].append(reddit_data)
    
    # AI åˆ†æ
    analysis = analyze_with_gemini(scraped_data)
    if analysis:
        scraped_data['ai_analysis'] = analysis
    
    # ä¿å­˜æŠ¥å‘Š
    save_report(scraped_data)
    
    print("\nâœ¨ æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆï¼")

if __name__ == '__main__':
    main()
